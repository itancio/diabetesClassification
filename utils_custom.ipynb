{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c61087ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.regularizers import L2\n",
    "\n",
    "from collections import defaultdict\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74e0c46",
   "metadata": {},
   "source": [
    "You can call this for the binary classification problem if you set `from_logits=False` \n",
    "when declaring the loss. With this, you will also not need to call the `tf.math.sigmoid()` function in the loopbecause the model output is already a probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c62202bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_models(rows_len, regu):\n",
    "    print(\"build_models\")\n",
    "    tf.random.set_seed(20)\n",
    "    \n",
    "    models = dict()\n",
    "\n",
    "    model_1 = Sequential(\n",
    "        [\n",
    "            keras.Input(shape=rows_len),\n",
    "            Dense(14, activation = 'relu', kernel_regularizer=L2(regu)),\n",
    "            Dense(12, activation = 'relu', kernel_regularizer=L2(regu)),\n",
    "            Dense(1, activation = 'sigmoid', kernel_regularizer=L2(regu))\n",
    "        ],\n",
    "        name='Model_1'\n",
    "    )\n",
    "    models['Model_1'] = model_1\n",
    "\n",
    "    model_2 = Sequential(\n",
    "        [\n",
    "            keras.Input(shape=rows_len),\n",
    "            Dense(20, activation = 'relu', kernel_regularizer=L2(regu)),\n",
    "            Dense(12, activation = 'relu', kernel_regularizer=L2(regu)),\n",
    "            Dense(12, activation = 'relu', kernel_regularizer=L2(regu)),\n",
    "            Dense(20, activation = 'relu', kernel_regularizer=L2(regu)),\n",
    "            Dense(1, activation = 'sigmoid', kernel_regularizer=L2(regu))\n",
    "        ],\n",
    "        name='Model_2'\n",
    "    )\n",
    "    models['Model_2'] = model_2\n",
    "    \n",
    "    model_3 = Sequential(\n",
    "        [\n",
    "            keras.Input(shape=rows_len),\n",
    "            Dense(32, activation = 'relu', kernel_regularizer=L2(regu)),\n",
    "            Dense(16, activation = 'relu', kernel_regularizer=L2(regu)),\n",
    "            Dense(8, activation = 'relu', kernel_regularizer=L2(regu)),\n",
    "            Dense(4, activation = 'relu', kernel_regularizer=L2(regu)),\n",
    "            Dense(12, activation = 'relu', kernel_regularizer=L2(regu)),\n",
    "            Dense(1, activation = 'sigmoid', kernel_regularizer=L2(regu))\n",
    "        ],\n",
    "        name='Model_3'\n",
    "    )\n",
    "    models['Model_3'] = model_3\n",
    "#     models = [model_1, model_2, model_3]\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f28b867e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_optimizer(degrees, datasets):\n",
    "    X_train, y_train, X_cv, y_cv, X_test, y_test = datasets\n",
    "    # Loop over 10 times. Each adding one more degree of polynomial higher than the last.\n",
    "    mses = defaultdict(list)\n",
    "    maps = defaultdict(list)\n",
    "    \n",
    "    \n",
    "    for degree in degrees:\n",
    "        # Add polynomial features to the training set\n",
    "        poly = PolynomialFeatures(degree, include_bias=False)\n",
    "        X_train_mapped = poly.fit_transform(X_train)\n",
    "\n",
    "        # Scale the training set\n",
    "        scaler_poly = MinMaxScaler()\n",
    "        X_train_mapped = scaler_poly.fit_transform(X_train_mapped)\n",
    "        maps['train'].append(X_train_mapped)\n",
    "\n",
    "        # Create and train the model\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train_mapped, y_train )\n",
    "        maps['models'].append(model)\n",
    "\n",
    "        # Compute the training MSE\n",
    "        yhat = model.predict(X_train_mapped)\n",
    "        train_mse = mean_squared_error(y_train, yhat) / 2\n",
    "        mses['train'].append(train_mse)\n",
    "\n",
    "        # Add polynomial features and scale the cross validation set\n",
    "        X_cv_mapped = poly.transform(X_cv)\n",
    "        X_cv_mapped = scaler_poly.transform(X_cv_mapped)\n",
    "        maps['cv'].append(X_cv_mapped)\n",
    "\n",
    "        # Compute the cross validation MSE\n",
    "        yhat = model.predict(X_cv_mapped)\n",
    "        cv_mse = mean_squared_error(y_cv, yhat) / 2\n",
    "        mses['cv'].append(cv_mse)\n",
    "\n",
    "        # Add polynomial features and scale the test set\n",
    "        X_test_mapped = poly.transform(X_test)\n",
    "        X_test_mapped = scaler_poly.transform(X_test_mapped)\n",
    "        maps['test'].append(X_test_mapped)\n",
    "\n",
    "        # Compute the test MSE\n",
    "        yhat = model.predict(X_test_mapped)\n",
    "        test_mse = mean_squared_error(y_test, yhat) / 2\n",
    "        mses['test'].append(test_mse)\n",
    "\n",
    "        print(degree, X_train_mapped.shape, X_cv_mapped.shape, X_test_mapped.shape)\n",
    "        \n",
    "    return maps, mses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53a862f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_boundary(model, X, y):\n",
    "    print(model, X.shape, y.shape)\n",
    "    # Convert actual output to numpy array\n",
    "    y_actual = np.array(y)\n",
    "\n",
    "    # Do prediction on test set\n",
    "    y_pred = model.predict(X)\n",
    "    y_pred = y_pred.squeeze()\n",
    "\n",
    "    # Search for an optimal value for the decision line\n",
    "    steps = range(10, 90, 1)\n",
    "    results = []\n",
    "    max_correct = val = 0\n",
    "\n",
    "    for step in steps:\n",
    "        x_val = step/100\n",
    "        y_vals = np.array([1 if x >= x_val else 0 for x in y_pred]) \n",
    "        ct = sum([int(pred == act) for pred, act in zip(y_vals, y_actual)])\n",
    "\n",
    "        results.append(ct)\n",
    "\n",
    "        if ct > max_correct:\n",
    "            max_correct = ct\n",
    "            val = x_val\n",
    "            \n",
    "    return val, max_correct, y_pred, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3367b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(x, y):\n",
    "    \n",
    "    # Get 60% of the dataset as the training set. Put the remaining 40% in temporary variables.\n",
    "    x_train, x_, y_train, y_ = train_test_split(x, y, test_size=0.40, random_state=80)\n",
    "\n",
    "    # Split the 40% subset above into two: one half for cross validation and the other for the test set\n",
    "    x_cv, x_test, y_cv, y_test = train_test_split(x_, y_, test_size=0.50, random_state=80)\n",
    "    \n",
    "    return x_train, y_train, x_cv, y_cv, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3229fcca",
   "metadata": {},
   "source": [
    "## Useful Methods for Plotting Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b634de60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_cv_test_mses(degrees, train_mses, cv_mses, test_mses, title):\n",
    "    plt.plot(degrees, train_mses, marker='o', c='r', label='training MSEs'); \n",
    "    plt.plot(degrees, cv_mses, marker='o', c='b', label='CV MSEs'); \n",
    "    plt.plot(degrees, test_mses, marker='o', c='g', label='test MSEs'); \n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"degree\"); \n",
    "    plt.ylabel(\"MSE\"); \n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7687d16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_data(diabetes_df, features):\n",
    "    \n",
    "    diabetes_df.hist(bins=10, figsize = (20, 20), color = 'g');\n",
    "\n",
    "    diabetes_pos_df = diabetes_df.query('Diagnosis == 1')\n",
    "    diabetes_pos_df.hist(bins=10, figsize=(20, 20), color = 'r');\n",
    "\n",
    " \n",
    "    sns.heatmap(diabetes_df.corr(), annot = True);\n",
    "\n",
    "    sns.pairplot(diabetes_df[features + ['Diagnosis']], hue=\"Diagnosis\")\n",
    "    \n",
    "    plt.title(\"3d Scatterplot\")\n",
    "    fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
    "\n",
    "    x = diabetes_df['FastingBloodSugar']\n",
    "    y = diabetes_df['HbA1c']\n",
    "    z = diabetes_df['Diagnosis']\n",
    "\n",
    "    ax.scatter(x, y, z)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7fa1b1",
   "metadata": {},
   "source": [
    "## Methods unused\n",
    "Not used in the lab. You can call this for the binary \n",
    "classification problem if you set `from_logits=False` \n",
    "when declaring the loss. With this, you will also not need\n",
    "to call the `tf.math.sigmoid()` function in the loop\n",
    "because the model output is already a probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "534ed536-aca4-4dc9-8e3b-8d077949045d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dataset(x, y, title):\n",
    "    plt.rcParams[\"figure.figsize\"] = (12,8)\n",
    "    plt.rcParams[\"lines.markersize\"] = 12\n",
    "    plt.scatter(x, y, marker='x', c='r'); \n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"x\"); \n",
    "    plt.ylabel(\"y\"); \n",
    "    plt.show()\n",
    "    \n",
    "def plot_train_cv_test(x_train, y_train, x_cv, y_cv, x_test, y_test, title):\n",
    "    plt.scatter(x_train, y_train, marker='x', c='r', label='training'); \n",
    "    plt.scatter(x_cv, y_cv, marker='o', c='b', label='cross validation'); \n",
    "    plt.scatter(x_test, y_test, marker='^', c='g', label='test'); \n",
    "    plt.title(\"input vs. target\")\n",
    "    plt.xlabel(\"x\"); \n",
    "    plt.ylabel(\"y\"); \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_bc_dataset(x, y, title):\n",
    "    for i in range(len(y)):\n",
    "        marker = 'x' if y[i] == 1 else 'o'\n",
    "        c = 'r' if y[i] == 1 else 'b'\n",
    "        plt.scatter(x[i,0], x[i,1], marker=marker, c=c); \n",
    "    plt.title(\"x1 vs x2\")\n",
    "    plt.xlabel(\"x1\"); \n",
    "    plt.ylabel(\"x2\"); \n",
    "    y_0 = mlines.Line2D([], [], color='r', marker='x', markersize=12, linestyle='None', label='y=1')\n",
    "    y_1 = mlines.Line2D([], [], color='b', marker='o', markersize=12, linestyle='None', label='y=0')\n",
    "    plt.title(title)\n",
    "    plt.legend(handles=[y_0, y_1])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def train_plot_poly(model, x_train, y_train, x_cv, y_cv, max_degree=10, baseline=None):\n",
    "    \n",
    "    train_mses = []\n",
    "    cv_mses = []\n",
    "    models = []\n",
    "    scalers = []\n",
    "    degrees = range(1,max_degree+1)\n",
    "\n",
    "    # Loop over 10 times. Each adding one more degree of polynomial higher than the last.\n",
    "    for degree in degrees:\n",
    "\n",
    "        # Add polynomial features to the training set\n",
    "        poly = PolynomialFeatures(degree, include_bias=False)\n",
    "        X_train_mapped = poly.fit_transform(x_train)\n",
    "\n",
    "        # Scale the training set\n",
    "        scaler_poly = StandardScaler()\n",
    "        X_train_mapped_scaled = scaler_poly.fit_transform(X_train_mapped)\n",
    "        scalers.append(scaler_poly)\n",
    "\n",
    "        # Create and train the model\n",
    "        model.fit(X_train_mapped_scaled, y_train )\n",
    "        models.append(model)\n",
    "\n",
    "        # Compute the training MSE\n",
    "        yhat = model.predict(X_train_mapped_scaled)\n",
    "        train_mse = mean_squared_error(y_train, yhat) / 2\n",
    "        train_mses.append(train_mse)\n",
    "\n",
    "        # Add polynomial features and scale the cross-validation set\n",
    "        poly = PolynomialFeatures(degree, include_bias=False)\n",
    "        X_cv_mapped = poly.fit_transform(x_cv)\n",
    "        X_cv_mapped_scaled = scaler_poly.transform(X_cv_mapped)\n",
    "\n",
    "        # Compute the cross-validation MSE\n",
    "        yhat = model.predict(X_cv_mapped_scaled)\n",
    "        cv_mse = mean_squared_error(y_cv, yhat) / 2\n",
    "        cv_mses.append(cv_mse)\n",
    "\n",
    "    # Plot the results\n",
    "    plt.plot(degrees, train_mses, marker='o', c='r', label='training MSEs'); \n",
    "    plt.plot(degrees, cv_mses, marker='o', c='b', label='CV MSEs'); \n",
    "    plt.plot(degrees, np.repeat(baseline, len(degrees)), linestyle='--', label='baseline')\n",
    "    plt.title(\"degree of polynomial vs. train and CV MSEs\")\n",
    "    plt.xticks(degrees)\n",
    "    plt.xlabel(\"degree\"); \n",
    "    plt.ylabel(\"MSE\"); \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def train_plot_reg_params(reg_params, x_train, y_train, x_cv, y_cv, degree= 1, baseline=None):\n",
    "    \n",
    "    train_mses = []\n",
    "    cv_mses = []\n",
    "    models = []\n",
    "    scalers = []\n",
    "\n",
    "    # Loop over 10 times. Each adding one more degree of polynomial higher than the last.\n",
    "    for reg_param in reg_params:\n",
    "\n",
    "        # Add polynomial features to the training set\n",
    "        poly = PolynomialFeatures(degree, include_bias=False)\n",
    "        X_train_mapped = poly.fit_transform(x_train)\n",
    "\n",
    "        # Scale the training set\n",
    "        scaler_poly = StandardScaler()\n",
    "        X_train_mapped_scaled = scaler_poly.fit_transform(X_train_mapped)\n",
    "        scalers.append(scaler_poly)\n",
    "\n",
    "        # Create and train the model\n",
    "        model = Ridge(alpha=reg_param)\n",
    "        model.fit(X_train_mapped_scaled, y_train)\n",
    "        models.append(model)\n",
    "\n",
    "        # Compute the training MSE\n",
    "        yhat = model.predict(X_train_mapped_scaled)\n",
    "        train_mse = mean_squared_error(y_train, yhat) / 2\n",
    "        train_mses.append(train_mse)\n",
    "\n",
    "        # Add polynomial features and scale the cross-validation set\n",
    "        poly = PolynomialFeatures(degree, include_bias=False)\n",
    "        X_cv_mapped = poly.fit_transform(x_cv)\n",
    "        X_cv_mapped_scaled = scaler_poly.transform(X_cv_mapped)\n",
    "\n",
    "        # Compute the cross-validation MSE\n",
    "        yhat = model.predict(X_cv_mapped_scaled)\n",
    "        cv_mse = mean_squared_error(y_cv, yhat) / 2\n",
    "        cv_mses.append(cv_mse)\n",
    "\n",
    "    # Plot the results\n",
    "    reg_params = [str(x) for x in reg_params]\n",
    "    plt.plot(reg_params, train_mses, marker='o', c='r', label='training MSEs'); \n",
    "    plt.plot(reg_params, cv_mses, marker='o', c='b', label='CV MSEs'); \n",
    "    plt.plot(reg_params, np.repeat(baseline, len(reg_params)), linestyle='--', label='baseline')\n",
    "    plt.title(\"lambda vs. train and CV MSEs\")\n",
    "    plt.xlabel(\"lambda\"); \n",
    "    plt.ylabel(\"MSE\"); \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def train_plot_diff_datasets(model, files, max_degree=10, baseline=None):\n",
    "    \n",
    "    for file in files:\n",
    "        \n",
    "        x_train, y_train, x_cv, y_cv, x_test, y_test = prepare_dataset(file['filename'])\n",
    "\n",
    "        train_mses = []\n",
    "        cv_mses = []\n",
    "        models = []\n",
    "        scalers = []\n",
    "        degrees = range(1,max_degree+1)\n",
    "\n",
    "        # Loop over 10 times. Each adding one more degree of polynomial higher than the last.\n",
    "        for degree in degrees:\n",
    "\n",
    "            # Add polynomial features to the training set\n",
    "            poly = PolynomialFeatures(degree, include_bias=False)\n",
    "            X_train_mapped = poly.fit_transform(x_train)\n",
    "\n",
    "            # Scale the training set\n",
    "            scaler_poly = StandardScaler()\n",
    "            X_train_mapped_scaled = scaler_poly.fit_transform(X_train_mapped)\n",
    "            scalers.append(scaler_poly)\n",
    "\n",
    "            # Create and train the model\n",
    "            model.fit(X_train_mapped_scaled, y_train )\n",
    "            models.append(model)\n",
    "\n",
    "            # Compute the training MSE\n",
    "            yhat = model.predict(X_train_mapped_scaled)\n",
    "            train_mse = mean_squared_error(y_train, yhat) / 2\n",
    "            train_mses.append(train_mse)\n",
    "\n",
    "            # Add polynomial features and scale the cross-validation set\n",
    "            poly = PolynomialFeatures(degree, include_bias=False)\n",
    "            X_cv_mapped = poly.fit_transform(x_cv)\n",
    "            X_cv_mapped_scaled = scaler_poly.transform(X_cv_mapped)\n",
    "\n",
    "            # Compute the cross-validation MSE\n",
    "            yhat = model.predict(X_cv_mapped_scaled)\n",
    "            cv_mse = mean_squared_error(y_cv, yhat) / 2\n",
    "            cv_mses.append(cv_mse)\n",
    "\n",
    "        # Plot the results\n",
    "        plt.plot(degrees, train_mses, marker='o', c='r', linestyle=file['linestyle'], label=f\"{file['label']} training MSEs\"); \n",
    "        plt.plot(degrees, cv_mses, marker='o', c='b', linestyle=file['linestyle'], label=f\"{file['label']} CV MSEs\"); \n",
    "\n",
    "    plt.plot(degrees, np.repeat(baseline, len(degrees)), linestyle='--', label='baseline')\n",
    "    plt.title(\"degree of polynomial vs. train and CV MSEs\")\n",
    "    plt.xticks(degrees)\n",
    "    plt.xlabel(\"degree\"); \n",
    "    plt.ylabel(\"MSE\"); \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def train_plot_learning_curve(model, x_train, y_train, x_cv, y_cv, degree= 1, baseline=None):\n",
    "    \n",
    "    train_mses = []\n",
    "    cv_mses = []\n",
    "    models = []\n",
    "    scalers = []\n",
    "    num_samples_train_and_cv = []\n",
    "    percents = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "\n",
    "    # Loop over 10 times. Each adding one more degree of polynomial higher than the last.\n",
    "    for percent in percents:\n",
    "        \n",
    "        num_samples_train = round(len(x_train) * (percent/100.0))\n",
    "        num_samples_cv = round(len(x_cv) * (percent/100.0))\n",
    "        num_samples_train_and_cv.append(num_samples_train + num_samples_cv)\n",
    "        \n",
    "        x_train_sub = x_train[:num_samples_train]\n",
    "        y_train_sub = y_train[:num_samples_train]\n",
    "        x_cv_sub = x_cv[:num_samples_cv]\n",
    "        y_cv_sub = y_cv[:num_samples_cv]\n",
    "\n",
    "        # Add polynomial features to the training set\n",
    "        poly = PolynomialFeatures(degree, include_bias=False)\n",
    "        X_train_mapped = poly.fit_transform(x_train_sub)\n",
    "\n",
    "        # Scale the training set\n",
    "        scaler_poly = StandardScaler()\n",
    "        X_train_mapped_scaled = scaler_poly.fit_transform(X_train_mapped)\n",
    "        scalers.append(scaler_poly)\n",
    "\n",
    "        # Create and train the model\n",
    "        model.fit(X_train_mapped_scaled, y_train_sub)\n",
    "        models.append(model)\n",
    "\n",
    "        # Compute the training MSE\n",
    "        yhat = model.predict(X_train_mapped_scaled)\n",
    "        train_mse = mean_squared_error(y_train_sub, yhat) / 2\n",
    "        train_mses.append(train_mse)\n",
    "\n",
    "        # Add polynomial features and scale the cross-validation set\n",
    "        poly = PolynomialFeatures(degree, include_bias=False)\n",
    "        X_cv_mapped = poly.fit_transform(x_cv_sub)\n",
    "        X_cv_mapped_scaled = scaler_poly.transform(X_cv_mapped)\n",
    "\n",
    "        # Compute the cross-validation MSE\n",
    "        yhat = model.predict(X_cv_mapped_scaled)\n",
    "        cv_mse = mean_squared_error(y_cv_sub, yhat) / 2\n",
    "        cv_mses.append(cv_mse)\n",
    "\n",
    "    # Plot the results\n",
    "    plt.plot(num_samples_train_and_cv, train_mses, marker='o', c='r', label='training MSEs'); \n",
    "    plt.plot(num_samples_train_and_cv, cv_mses, marker='o', c='b', label='CV MSEs'); \n",
    "    plt.plot(num_samples_train_and_cv, np.repeat(baseline, len(percents)), linestyle='--', label='baseline')\n",
    "    plt.title(\"number of examples vs. train and CV MSEs\")\n",
    "    plt.xlabel(\"total number of training and cv examples\"); \n",
    "    plt.ylabel(\"MSE\"); \n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
